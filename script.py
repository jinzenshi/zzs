import requests
import json
import datetime
from datetime import timedelta
import os
import urllib3
import ssl
import re  # æ–°å¢ï¼šç”¨äºæ˜“æ–¹è¾¾æ•°æ®æ­£åˆ™è§£æ
from bs4 import BeautifulSoup
from requests.adapters import HTTPAdapter

# ç¦ç”¨ SSL è­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# ==========================================
# æ ¸å¿ƒä¿®å¤ï¼šSSL é€‚é…å™¨ (è§£å†³ Hostname/Cert å†²çª)
# ==========================================
class LegacySSLAdapter(HTTPAdapter):
    """
    1. å¼ºåˆ¶å¼€å¯ OP_LEGACY_SERVER_CONNECT (è§£å†³ Unsafe Legacy Renegotiation)
    2. æ˜¾å¼ç¦ç”¨ check_hostname (è§£å†³ Cannot set verify_mode to CERT_NONE)
    """
    def init_poolmanager(self, connections, maxsize, block=False):
        ctx = ssl.create_default_context(ssl.Purpose.SERVER_AUTH)
        
        # --- å…³é”®ä¿®å¤å¼€å§‹ ---
        # å¿…é¡»æ˜¾å¼å…³é—­ hostname æ£€æŸ¥ï¼Œå¦åˆ™ Python ä¸å…è®¸å°† verify_mode è®¾ä¸º CERT_NONE
        ctx.check_hostname = False
        ctx.verify_mode = ssl.CERT_NONE
        # --- å…³é”®ä¿®å¤ç»“æŸ ---
        
        # å…è®¸æ—§ç‰ˆä¸å®‰å…¨è¿æ¥
        ctx.options |= 0x4  # OP_LEGACY_SERVER_CONNECT
        # å…è®¸ä½å®‰å…¨çº§åˆ«çš„åŠ å¯†å¥—ä»¶
        ctx.set_ciphers('DEFAULT@SECLEVEL=1')
        
        self.poolmanager = urllib3.poolmanager.PoolManager(
            num_pools=connections,
            maxsize=maxsize,
            block=block,
            ssl_context=ctx
        )

# ==========================================
# é…ç½®åŒºåŸŸ (ä» GitHub Secrets è¯»å–)
# ==========================================
FEISHU_CONFIG = {
    "APP_ID": "cli_a9aac56abc78dbde",
    "APP_SECRET": "zYsXkFulzxMCrqnAjvPTiyVUWCIKFwS5",
    "APP_TOKEN": "Qurjbd950a7XzIsMFZrclwn5n9d",
    "TABLE_ID": "tblHts4IwRE8WCBB"
}

# ==========================================
# é£ä¹¦ API æ¨¡å—
# ==========================================
class FeishuClient:
    def __init__(self, app_id, app_secret):
        self.app_id = app_id
        self.app_secret = app_secret
        self.token = None
        self.token_expire_time = 0

    def get_tenant_access_token(self):
        if not self.app_id or not self.app_secret:
            print("âŒ é”™è¯¯: ç¯å¢ƒå˜é‡ç¼ºå¤±ï¼Œè¯·æ£€æŸ¥ GitHub Secretsã€‚")
            return None
        if self.token and datetime.datetime.now().timestamp() < self.token_expire_time - 600:
            return self.token

        url = "https://open.feishu.cn/open-apis/auth/v3/tenant_access_token/internal"
        headers = {"Content-Type": "application/json; charset=utf-8"}
        data = {"app_id": self.app_id, "app_secret": self.app_secret}
        try:
            response = requests.post(url, headers=headers, json=data)
            resp_json = response.json()
            if resp_json.get("code") == 0:
                self.token = resp_json.get("tenant_access_token")
                self.token_expire_time = datetime.datetime.now().timestamp() + resp_json.get("expire", 7200)
                return self.token
            else:
                print(f"[Feishu Auth Error] {resp_json}")
                return None
        except Exception as e:
            print(f"[Feishu Auth Exception] {e}")
            return None

    def add_record(self, app_token, table_id, fields):
        token = self.get_tenant_access_token()
        if not token: return False
        
        url = f"https://open.feishu.cn/open-apis/bitable/v1/apps/{app_token}/tables/{table_id}/records"
        headers = {"Authorization": f"Bearer {token}", "Content-Type": "application/json; charset=utf-8"}
        payload = {"fields": fields}
        try:
            response = requests.post(url, headers=headers, json=payload)
            if response.json().get("code") == 0:
                print(f"âœ… æˆåŠŸå†™å…¥: {fields.get('äº§å“ä»£ç ')}")
                return True
            else:
                print(f"âŒ å†™å…¥å¤±è´¥: {response.text}")
                return False
        except Exception as e:
            print(f"âŒ è¯·æ±‚å¼‚å¸¸: {e}")
            return False

# ==========================================
# å·¥å…·å‡½æ•°
# ==========================================
def load_purchase_dates(filename="è´­å…¥æ—¥æœŸ.txt"):
    info_map = {}
    if not os.path.exists(filename):
        print(f"âš ï¸ è­¦å‘Š: æœªæ‰¾åˆ° {filename}ï¼Œè¯·ç¡®ä¿å·²å°†æ­¤æ–‡ä»¶ä¸Šä¼ åˆ° GitHub ä»“åº“æ ¹ç›®å½•ã€‚")
        return info_map
    try:
        with open(filename, 'r', encoding='utf-8') as f:
            for line in f:
                parts = line.strip().split()
                if len(parts) >= 2 and not line.startswith(("{", "}", "source")):
                    code = parts[0].strip()
                    try:
                        c_date = datetime.datetime.strptime(parts[1].strip(), "%Y-%m-%d").date()
                        r_date = None
                        if len(parts) >= 3:
                            try:
                                r_date = datetime.datetime.strptime(parts[2].strip(), "%Y-%m-%d").date()
                            except: pass
                        info_map[code] = {'confirm_date': c_date, 'redeem_date': r_date}
                    except: pass
    except Exception as e:
        print(f"è¯»å–æ–‡ä»¶é”™è¯¯: {e}")
    return info_map

def load_product_codes(filename):
    """ä»æ–‡ä»¶åŠ è½½äº§å“ä»£ç åˆ—è¡¨"""
    codes = []
    if not os.path.exists(filename):
        print(f"âš ï¸ è­¦å‘Š: æœªæ‰¾åˆ° {filename}")
        return codes
    try:
        with open(filename, 'r', encoding='utf-8') as f:
            codes = [line.strip() for line in f if line.strip()]
    except Exception as e:
        print(f"è¯»å– {filename} é”™è¯¯: {e}")
    return codes

def get_30_day_prior_record(sorted_data, latest_date):
    """
    è·å–30å¤©å‰çš„è®°å½•ã€‚
    å¦‚æœæ­£å¥½30å¤©å‰æ— æ•°æ®ï¼Œé€»è¾‘æ˜¯â€œé¡ºå»¶â€ï¼Œå³æ‰¾ >= (latest_date - 30) çš„ç¬¬ä¸€æ¡æ•°æ®ã€‚
    """
    target_date = latest_date - timedelta(days=30)
    for item in sorted_data:
        if item['date'] >= target_date: return item
    return None

def get_nav_for_date(sorted_data, target_date):
    if not target_date: return None
    for item in sorted_data:
        if item['date'] >= target_date: return item['nav']
    return 0

# ==========================================
# çˆ¬è™«é€»è¾‘ (å« SSL ä¿®å¤)
# ==========================================
def query_bocom(product_code, purchase_date=None, redeem_date=None):
    """äº¤é€šé“¶è¡Œ"""
    url = "https://www.bocommwm.com/SITE/queryJylcBreakDetail.do"
    headers = {"User-Agent": "Mozilla/5.0", "X-Requested-With": "XMLHttpRequest"}
    cookies = {"JSESSIONID": "8D2A39697E6E2A04E0B05229A3E75237"}
    payload = {"REQ_MESSAGE": json.dumps({
        "REQ_HEAD": {"TRAN_PROCESS": "", "TRAN_ID": ""},
        "REQ_BODY": {"c_fundcode": product_code, "c_interestway": "0", "c_productcode": "undefined", "type": "max"}
    })}
    try:
        res = requests.post(url, headers=headers, cookies=cookies, data=payload, verify=False, timeout=10)
        data = res.json().get("RSP_BODY", {}).get("result", {}).get("profitList", [])
        clean = []
        for i in data:
            try:
                clean.append({'date': datetime.datetime.strptime(i['d_cdate'], '%Y-%m-%d').date(), 'nav': float(i['f_netvalue'])})
            except: continue
        clean.sort(key=lambda x: x['date'])
        if not clean: return product_code, "No Data", "No Data", None, 0, 0
        
        last = clean[-1]
        prior = get_30_day_prior_record(clean, last['date'])
        return product_code, last['nav'], prior['nav'] if prior else 0, last['date'], get_nav_for_date(clean, purchase_date), get_nav_for_date(clean, redeem_date)
    except: return product_code, "Error", "Error", None, 0, 0

def query_cmbc_fuzhu(product_code, product_name=None, purchase_date=None, redeem_date=None):
    """æ°‘ç”Ÿç†è´¢ (åº”ç”¨æ·±åº¦ SSL ä¿®å¤)"""
    if not product_name:
        product_name = product_code
    url = "https://www.cmbcwm.com.cn/gw/po_web/BTADailyQry"
    headers = {'User-Agent': 'Mozilla/5.0'}
    start_date = (datetime.datetime.now() - timedelta(days=1460)).strftime("%Y%m%d")
    payload = {'chart_type': '0', 'real_prd_code': product_code, 'begin_date': start_date, 'end_date': ''}

    try:
        # === å…³é”®ä¿®å¤ï¼šæŒ‚è½½ Adapter ===
        session = requests.Session()
        session.mount('https://', LegacySSLAdapter())

        # verify=False ä»ç„¶ä¿ç•™ï¼Œä½†ç°åœ¨ adapter å†…éƒ¨å·²ç»å¤„ç†å¥½äº† check_hostname=False
        res = session.post(url, headers=headers, data=payload, verify=False, timeout=15)
        # ===========================

        nav_list = res.json().get('list', [])
        clean = []
        for i in nav_list:
            try:
                clean.append({'date': datetime.datetime.strptime(str(i['ISS_DATE']), "%Y%m%d").date(), 'nav': float(i['NAV'])})
            except: continue
        clean.sort(key=lambda x: x['date'])
        if not clean: return product_name, "No Data", "No Data", None, 0, 0

        last = clean[-1]
        prior = get_30_day_prior_record(clean, last['date'])
        return product_name, last['nav'], prior['nav'] if prior else 0, last['date'], get_nav_for_date(clean, purchase_date), get_nav_for_date(clean, redeem_date)
    except Exception as e:
        print(f"æ°‘ç”Ÿå¼‚å¸¸: {e}")
        return product_name, "Error", "Error", None, 0, 0

def query_efunds_yizeng(product_code, purchase_date=None, redeem_date=None):
    """æ˜“æ–¹è¾¾ (æ•´åˆäº†æ–°ç‰ˆé€»è¾‘ï¼šåˆå¹¶å†å²ä¸è¿‘æœŸæ•°æ®ï¼Œä½¿ç”¨æ­£åˆ™è§£æ)"""
    url_history = f'https://cdn.efunds.com.cn/market/2.0/his/{product_code}_all.js'
    url_recent = f'https://cdn.efunds.com.cn/market/2.0/{product_code}_1y.js'

    headers = {
        'Referer': 'https://www.efunds.com.cn/',
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36'
    }

    session = requests.Session()
    session.headers.update(headers)
    merged_data = {}

    try:
        for url in [url_history, url_recent]:
            try:
                res = session.get(url, timeout=10)
                # ä½¿ç”¨æ­£åˆ™æå–å¼•å·å†…çš„å†…å®¹
                match = re.search(r'=\s*"(.*?)";', res.text)
                if match:
                    content = match.group(1)
                    records = content.split(';')
                    for record in records:
                        if not record or '_' not in record or record.startswith('0_'):
                            continue
                        parts = record.split('_')
                        if len(parts) >= 3:
                            # parts[0]: YYYYMMDD, parts[2]: NAV
                            d_str = parts[0]
                            nav_val = float(parts[2])
                            dt = datetime.datetime.strptime(d_str, "%Y%m%d").date()
                            merged_data[dt] = nav_val
            except Exception as e:
                print(f"æ˜“æ–¹è¾¾ URL è¯·æ±‚é”™è¯¯ {url}: {e}")
                continue

        # è½¬æ¢ä¸ºåˆ—è¡¨å¹¶æ’åº
        clean = [{'date': k, 'nav': v} for k, v in merged_data.items()]
        clean.sort(key=lambda x: x['date'])

        if not clean:
            return product_code, "No Data", "No Data", None, 0, 0

        last = clean[-1]
        prior = get_30_day_prior_record(clean, last['date'])

        return product_code, last['nav'], prior['nav'] if prior else 0, last['date'], get_nav_for_date(clean, purchase_date), get_nav_for_date(clean, redeem_date)

    except Exception as e:
        print(f"æ˜“æ–¹è¾¾å¤„ç†å¼‚å¸¸: {e}")
        return product_code, "Error", "Error", None, 0, 0

def query_citic_wealth(product_code, purchase_date=None, redeem_date=None):
    """ä¸­ä¿¡é“¶è¡Œ (å®‰ç›ˆè±¡) - æ•´åˆè‡ª å®‰ç›ˆè±¡...py"""
    url = "https://wechat.citic-wealth.com/cms.product/api/custom/productInfo/getTAProductNav"
    params = {
        "prodCode": product_code,
        "queryUnit": "5" # æŸ¥è¯¢è¿‘5å¹´? æˆ–è€…å•ä½ï¼ŒåŸè„šæœ¬ä¸º5
    }
    
    headers = {
        "Accept": "application/json, text/plain, */*",
        "Origin": "https://www.citic-wealth.com",
        "Referer": "https://www.citic-wealth.com/",
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36",
        "channel": "h5_trade_service"
    }
    
    # æ³¨æ„ï¼šJSESSIONID å¯èƒ½ä¼šè¿‡æœŸï¼Œå¦‚æœå¤±æ•ˆéœ€è¦åœ¨ GitHub Secrets æˆ–é…ç½®æ–‡ä»¶ä¸­æ›´æ–°
    cookies = {
        "JSESSIONID": "rumlKZY0aDXcnkUn8dvr_lIyapa4KANRZMNXpQ3o"
    }

    try:
        # ä½¿ç”¨ LegacySSLAdapter ä»¥é˜²ä¸‡ä¸€ï¼Œè™½ç„¶ä¸­ä¿¡ä¸€èˆ¬ä¸éœ€è¦
        session = requests.Session()
        session.mount('https://', LegacySSLAdapter())
        
        response = session.get(url, headers=headers, params=params, cookies=cookies, verify=False, timeout=10)
        data = response.json()
        
        if data.get("code") != "0000":
            print(f"ä¸­ä¿¡ API é”™è¯¯ ({product_code}): {data.get('msg')}")
            return product_code, "API Error", 0, None, 0, 0

        nav_list = data.get("data", {}).get("productNavList", [])
        clean = []
        for item in nav_list:
            date_str = item.get("navDate")
            nav_value = item.get("nav")
            if date_str and nav_value is not None:
                try:
                    dt = datetime.datetime.strptime(date_str, "%Y%m%d").date()
                    clean.append({'date': dt, 'nav': float(nav_value)})
                except: continue
        
        clean.sort(key=lambda x: x['date'])
        
        if not clean:
            return product_code, "No Data", "No Data", None, 0, 0

        last = clean[-1]
        prior = get_30_day_prior_record(clean, last['date'])
        
        return product_code, last['nav'], prior['nav'] if prior else 0, last['date'], get_nav_for_date(clean, purchase_date), get_nav_for_date(clean, redeem_date)

    except Exception as e:
        print(f"ä¸­ä¿¡é“¶è¡Œå¼‚å¸¸ ({product_code}): {e}")
        return product_code, "Error", "Error", None, 0, 0

def query_hzbank(product_code, product_name=None, purchase_date=None, redeem_date=None):
    """æ­é“¶ç†è´¢"""
    if not product_name:
        product_name = product_code
    # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦æ ¹æ®æ­é“¶çš„å®é™…APIè°ƒæ•´URLå’Œè¯·æ±‚æ–¹å¼
    # ç›®å‰æš‚æ—¶è¿”å›é”™è¯¯ä¿¡æ¯ï¼Œæç¤ºéœ€è¦å®ç°
    print(f"âš ï¸ æ­é“¶ç†è´¢æŸ¥è¯¢åŠŸèƒ½å¾…å®ç° (äº§å“ä»£ç : {product_code})")
    return product_name, "Not Implemented", "Not Implemented", None, 0, 0

def query_boc_niannianxin(purchase_date=None, redeem_date=None):
    """ä¸­è¡Œ"""
    code, name = "2501240100", "å¹´å¹´é‘«æœ€çŸ­æŒæœ‰æœŸ11å·A"
    url = "https://www.bankofchina.com/sourcedb/srfd6_2024/index_2.html"
    try:
        res = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=10)
        res.encoding = 'utf-8'
        soup = BeautifulSoup(res.text, 'html.parser')
        for row in soup.find_all('tr'):
            cols = row.find_all('td')
            if len(cols) >= 3 and cols[1].get_text(strip=True) == name:
                return code, float(cols[2].get_text(strip=True)), 0, datetime.date.today(), 0, 0
    except: pass
    return code, "Error", 0, None, 0, 0

# ==========================================
# ä¸»ç¨‹åº
# ==========================================
def main():
    print("ğŸš€ å¼€å§‹è¿è¡Œ...")
    info_map = load_purchase_dates("è´­å…¥æ—¥æœŸ.txt")
    feishu = FeishuClient(FEISHU_CONFIG["APP_ID"], FEISHU_CONFIG["APP_SECRET"])

    tasks = []

    # 1. äº¤è¡Œäº§å“
    print("ğŸ“‚ åŠ è½½äº¤è¡Œäº§å“ä»£ç ...")
    bocom_codes = load_product_codes("/Users/cyhuang/Desktop/curl/äº¤è¡Œäº§å“ä»£ç .txt")
    for c in bocom_codes:
        d = info_map.get(c, {})
        tasks.append((query_bocom(c, d.get('confirm_date'), d.get('redeem_date')), d.get('confirm_date')))

    # 2. æ°‘ç”Ÿäº§å“
    print("ğŸ“‚ åŠ è½½æ°‘ç”Ÿäº§å“ä»£ç ...")
    cmbc_codes = load_product_codes("/Users/cyhuang/Desktop/curl/æ°‘ç”Ÿäº§å“ä»£ç .txt")
    for c in cmbc_codes:
        d = info_map.get(c, {})
        tasks.append((query_cmbc_fuzhu(c, c, d.get('confirm_date'), d.get('redeem_date')), d.get('confirm_date')))

    # 3. æ˜“æ–¹è¾¾äº§å“
    print("ğŸ“‚ åŠ è½½æ˜“æ–¹è¾¾äº§å“ä»£ç ...")
    efunds_codes = load_product_codes("/Users/cyhuang/Desktop/curl/æ˜“æ–¹è¾¾äº§å“ä»£ç .txt")
    for c in efunds_codes:
        d = info_map.get(c, {})
        tasks.append((query_efunds_yizeng(c, d.get('confirm_date'), d.get('redeem_date')), d.get('confirm_date')))

    # 4. ä¸­ä¿¡é“¶è¡Œäº§å“
    print("ğŸ“‚ åŠ è½½ä¸­ä¿¡é“¶è¡Œäº§å“ä»£ç ...")
    citic_codes = load_product_codes("/Users/cyhuang/Desktop/curl/ä¸­ä¿¡é“¶è¡Œäº§å“ä»£ç .txt")
    for c in citic_codes:
        d = info_map.get(c, {})
        tasks.append((query_citic_wealth(c, d.get('confirm_date'), d.get('redeem_date')), d.get('confirm_date')))

    # 5. æ­é“¶äº§å“
    print("ğŸ“‚ åŠ è½½æ­é“¶äº§å“ä»£ç ...")
    hzbank_codes = load_product_codes("/Users/cyhuang/Desktop/curl/æ­é“¶äº§å“ä»£ç .txt")
    for c in hzbank_codes:
        d = info_map.get(c, {})
        tasks.append((query_hzbank(c, c, d.get('confirm_date'), d.get('redeem_date')), d.get('confirm_date')))

    # 6. ä¸­è¡Œ (å›ºå®šäº§å“)
    print("ğŸ“‚ åŠ è½½ä¸­è¡Œäº§å“...")
    boc_dates = info_map.get("2501240100", {})
    tasks.append((query_boc_niannianxin(boc_dates.get('confirm_date'), boc_dates.get('redeem_date')), boc_dates.get('confirm_date')))

    # æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡å¹¶å†™å…¥é£ä¹¦
    for (res, specific_c_date) in tasks:
        code, cur, prior, date_obj, pur, red = res
        if isinstance(cur, (int, float)) and date_obj:
            ts = int(datetime.datetime.combine(date_obj, datetime.time.min).timestamp() * 1000)
            c_ts = int(datetime.datetime.combine(specific_c_date, datetime.time.min).timestamp() * 1000) if specific_c_date else None

            fields = {
                "äº§å“ä»£ç ": code,
                "å½“æ—¥å‡€å€¼": cur,
                "30æ—¥å‰å‡€å€¼": prior,
                "è´­å…¥å½“æ—¥å‡€å€¼": pur,
                "èµå›å‡€å€¼": red if isinstance(red, (int, float)) else 0,
                "ç¡®è®¤æ—¥": c_ts,
                "æ•°æ®æ›´æ–°æ—¥æœŸ": ts
            }
            feishu.add_record(FEISHU_CONFIG["APP_TOKEN"], FEISHU_CONFIG["TABLE_ID"], fields)
        else:
            print(f"âš ï¸ è·³è¿‡: {code} (è·å–å¤±è´¥æˆ–æ ¼å¼é”™è¯¯)")

if __name__ == "__main__":
    main()
