import requests
import json
import datetime
from datetime import timedelta
import os
import urllib3
import ssl
import re  # æ–°å¢ï¼šç”¨äºæ˜“æ–¹è¾¾æ•°æ®æ­£åˆ™è§£æ
from bs4 import BeautifulSoup
from requests.adapters import HTTPAdapter

# ç¦ç”¨ SSL è­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# ==========================================
# æ ¸å¿ƒä¿®å¤ï¼šSSL é€‚é…å™¨ (è§£å†³ Hostname/Cert å†²çª)
# ==========================================
class LegacySSLAdapter(HTTPAdapter):
    """
    1. å¼ºåˆ¶å¼€å¯ OP_LEGACY_SERVER_CONNECT (è§£å†³ Unsafe Legacy Renegotiation)
    2. æ˜¾å¼ç¦ç”¨ check_hostname (è§£å†³ Cannot set verify_mode to CERT_NONE)
    """
    def init_poolmanager(self, connections, maxsize, block=False):
        ctx = ssl.create_default_context(ssl.Purpose.SERVER_AUTH)
        
        # --- å…³é”®ä¿®å¤å¼€å§‹ ---
        # å¿…é¡»æ˜¾å¼å…³é—­ hostname æ£€æŸ¥ï¼Œå¦åˆ™ Python ä¸å…è®¸å°† verify_mode è®¾ä¸º CERT_NONE
        ctx.check_hostname = False
        ctx.verify_mode = ssl.CERT_NONE
        # --- å…³é”®ä¿®å¤ç»“æŸ ---
        
        # å…è®¸æ—§ç‰ˆä¸å®‰å…¨è¿æ¥
        ctx.options |= 0x4  # OP_LEGACY_SERVER_CONNECT
        # å…è®¸ä½å®‰å…¨çº§åˆ«çš„åŠ å¯†å¥—ä»¶
        ctx.set_ciphers('DEFAULT@SECLEVEL=1')
        
        self.poolmanager = urllib3.poolmanager.PoolManager(
            num_pools=connections,
            maxsize=maxsize,
            block=block,
            ssl_context=ctx
        )

# ==========================================
# é…ç½®åŒºåŸŸ (ä» GitHub Secrets è¯»å–)
# ==========================================
FEISHU_CONFIG = {
    "APP_ID": "cli_a9aac56abc78dbde",
    "APP_SECRET": "zYsXkFulzxMCrqnAjvPTiyVUWCIKFwS5",
    "APP_TOKEN": "Qurjbd950a7XzIsMFZrclwn5n9d",
    "TABLE_ID": "tblHts4IwRE8WCBB"
}

# ==========================================
# é£ä¹¦ API æ¨¡å—
# ==========================================
class FeishuClient:
    def __init__(self, app_id, app_secret):
        self.app_id = app_id
        self.app_secret = app_secret
        self.token = None
        self.token_expire_time = 0

    def get_tenant_access_token(self):
        if not self.app_id or not self.app_secret:
            print("âŒ é”™è¯¯: ç¯å¢ƒå˜é‡ç¼ºå¤±ï¼Œè¯·æ£€æŸ¥ GitHub Secretsã€‚")
            return None
        if self.token and datetime.datetime.now().timestamp() < self.token_expire_time - 600:
            return self.token

        url = "https://open.feishu.cn/open-apis/auth/v3/tenant_access_token/internal"
        headers = {"Content-Type": "application/json; charset=utf-8"}
        data = {"app_id": self.app_id, "app_secret": self.app_secret}
        try:
            response = requests.post(url, headers=headers, json=data)
            resp_json = response.json()
            if resp_json.get("code") == 0:
                self.token = resp_json.get("tenant_access_token")
                self.token_expire_time = datetime.datetime.now().timestamp() + resp_json.get("expire", 7200)
                return self.token
            else:
                print(f"[Feishu Auth Error] {resp_json}")
                return None
        except Exception as e:
            print(f"[Feishu Auth Exception] {e}")
            return None

    def add_record(self, app_token, table_id, fields):
        token = self.get_tenant_access_token()
        if not token: return False

        url = f"https://open.feishu.cn/open-apis/bitable/v1/apps/{app_token}/tables/{table_id}/records"
        headers = {"Authorization": f"Bearer {token}", "Content-Type": "application/json; charset=utf-8"}
        payload = {"fields": fields}
        try:
            response = requests.post(url, headers=headers, json=payload)
            if response.json().get("code") == 0:
                print(f"âœ… æˆåŠŸå†™å…¥: {fields.get('äº§å“ä»£ç ')}")
                return True
            else:
                print(f"âŒ å†™å…¥å¤±è´¥: {response.text}")
                return False
        except Exception as e:
            print(f"âŒ è¯·æ±‚å¼‚å¸¸: {e}")
            return False

    def clear_table(self, app_token, table_id):
        """æ¸…ç©ºè¡¨æ ¼æ‰€æœ‰è®°å½•"""
        token = self.get_tenant_access_token()
        if not token: return False

        # 1. è·å–æ‰€æœ‰è®°å½•
        list_url = f"https://open.feishu.cn/open-apis/bitable/v1/apps/{app_token}/tables/{table_id}/records"
        headers = {"Authorization": f"Bearer {token}", "Content-Type": "application/json; charset=utf-8"}

        try:
            # è·å–æ‰€æœ‰è®°å½•ï¼ˆåˆ†é¡µè·å–ï¼‰
            all_records = []
            page_token = ""
            while True:
                params = {"page_size": 500}  # æ¯æ¬¡æœ€å¤š500æ¡
                if page_token:
                    params["page_token"] = page_token

                response = requests.get(list_url, headers=headers, params=params)
                resp_data = response.json()

                if resp_data.get("code") != 0:
                    print(f"âŒ è·å–è®°å½•å¤±è´¥: {resp_data.get('msg')}")
                    return False

                records = resp_data.get("data", {}).get("items", [])
                all_records.extend(records)

                # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰ä¸‹ä¸€é¡µ
                has_more = resp_data.get("data", {}).get("has_more", False)
                page_token = resp_data.get("data", {}).get("page_token", "")
                if not has_more or not page_token:
                    break

            if not all_records:
                print("â„¹ï¸ è¡¨æ ¼å·²ç»æ˜¯ç©ºçš„")
                return True

            # 2. æ‰¹é‡åˆ é™¤è®°å½•ï¼ˆæœ€å¤š100æ¡ä¸€æ‰¹ï¼‰
            delete_url = f"https://open.feishu.cn/open-apis/bitable/v1/apps/{app_token}/tables/{table_id}/records/batch_delete"
            batch_size = 100

            for i in range(0, len(all_records), batch_size):
                batch = all_records[i:i + batch_size]
                record_ids = [record["record_id"] for record in batch]

                payload = {"records": record_ids}
                response = requests.post(delete_url, headers=headers, json=payload)
                resp_data = response.json()

                if resp_data.get("code") != 0:
                    print(f"âŒ åˆ é™¤æ‰¹æ¬¡ {i//batch_size + 1} å¤±è´¥: {resp_data.get('msg')}")
                    return False

            print(f"âœ… æˆåŠŸæ¸…ç©ºè¡¨æ ¼ï¼Œå…±åˆ é™¤ {len(all_records)} æ¡è®°å½•")
            return True

        except Exception as e:
            print(f"âŒ æ¸…ç©ºè¡¨æ ¼å¼‚å¸¸: {e}")
            return False

# ==========================================
# å·¥å…·å‡½æ•°
# ==========================================
def load_purchase_dates(filename="è´­å…¥æ—¥æœŸ.txt"):
    """
    ä»æ–‡ä»¶åŠ è½½è´­å…¥æ—¥æœŸä¿¡æ¯
    æ”¯æŒä¸€ä¸ªäº§å“å¤šä¸ªè´­ä¹°æ—¥æœŸï¼Œè¿”å›å­—å…¸ï¼Œå€¼ä¸ºåˆ—è¡¨
    """
    info_map = {}
    if not os.path.exists(filename):
        print(f"âš ï¸ è­¦å‘Š: æœªæ‰¾åˆ° {filename}ï¼Œè¯·ç¡®ä¿å·²å°†æ­¤æ–‡ä»¶ä¸Šä¼ åˆ° GitHub ä»“åº“æ ¹ç›®å½•ã€‚")
        return info_map
    try:
        with open(filename, 'r', encoding='utf-8') as f:
            for line in f:
                parts = line.strip().split()
                if len(parts) >= 2 and not line.startswith(("{", "}", "source")):
                    code = parts[0].strip()
                    try:
                        c_date = datetime.datetime.strptime(parts[1].strip(), "%Y-%m-%d").date()
                        r_date = None
                        if len(parts) >= 3:
                            try:
                                r_date = datetime.datetime.strptime(parts[2].strip(), "%Y-%m-%d").date()
                            except: pass

                        # ä½¿ç”¨åˆ—è¡¨å­˜å‚¨å¤šä¸ªè´­ä¹°æ—¥æœŸ
                        if code not in info_map:
                            info_map[code] = []
                        info_map[code].append({'confirm_date': c_date, 'redeem_date': r_date})
                    except: pass
    except Exception as e:
        print(f"è¯»å–æ–‡ä»¶é”™è¯¯: {e}")
    return info_map

def load_product_codes(filename):
    """ä»æ–‡ä»¶åŠ è½½äº§å“ä»£ç åˆ—è¡¨"""
    codes = []
    if not os.path.exists(filename):
        print(f"âš ï¸ è­¦å‘Š: æœªæ‰¾åˆ° {filename}")
        return codes
    try:
        with open(filename, 'r', encoding='utf-8') as f:
            codes = [line.strip() for line in f if line.strip()]
    except Exception as e:
        print(f"è¯»å– {filename} é”™è¯¯: {e}")
    return codes

def get_30_day_prior_record(sorted_data, latest_date):
    """
    è·å–30å¤©å‰çš„è®°å½•ã€‚
    å¦‚æœæ­£å¥½30å¤©å‰æ— æ•°æ®ï¼Œé€»è¾‘æ˜¯â€œé¡ºå»¶â€ï¼Œå³æ‰¾ >= (latest_date - 30) çš„ç¬¬ä¸€æ¡æ•°æ®ã€‚
    """
    target_date = latest_date - timedelta(days=30)
    for item in sorted_data:
        if item['date'] >= target_date: return item
    return None

def get_nav_for_date(sorted_data, target_date):
    if not target_date: return None
    for item in sorted_data:
        if item['date'] >= target_date: return item['nav']
    return 0

# ==========================================
# çˆ¬è™«é€»è¾‘ (å« SSL ä¿®å¤)
# ==========================================
def query_bocom(product_code, purchase_date=None, redeem_date=None):
    """äº¤é€šé“¶è¡Œ"""
    url = "https://www.bocommwm.com/SITE/queryJylcBreakDetail.do"
    headers = {"User-Agent": "Mozilla/5.0", "X-Requested-With": "XMLHttpRequest"}
    cookies = {"JSESSIONID": "8D2A39697E6E2A04E0B05229A3E75237"}
    payload = {"REQ_MESSAGE": json.dumps({
        "REQ_HEAD": {"TRAN_PROCESS": "", "TRAN_ID": ""},
        "REQ_BODY": {"c_fundcode": product_code, "c_interestway": "0", "c_productcode": "undefined", "type": "max"}
    })}
    try:
        res = requests.post(url, headers=headers, cookies=cookies, data=payload, verify=False, timeout=10)
        data = res.json().get("RSP_BODY", {}).get("result", {}).get("profitList", [])
        clean = []
        for i in data:
            try:
                clean.append({'date': datetime.datetime.strptime(i['d_cdate'], '%Y-%m-%d').date(), 'nav': float(i['f_netvalue'])})
            except: continue
        clean.sort(key=lambda x: x['date'])
        if not clean: return product_code, "No Data", "No Data", None, 0, 0
        
        last = clean[-1]
        prior = get_30_day_prior_record(clean, last['date'])
        return product_code, last['nav'], prior['nav'] if prior else 0, last['date'], get_nav_for_date(clean, purchase_date), get_nav_for_date(clean, redeem_date)
    except: return product_code, "Error", "Error", None, 0, 0

def query_cmbc_fuzhu(product_code, product_name=None, purchase_date=None, redeem_date=None):
    """æ°‘ç”Ÿç†è´¢ (åº”ç”¨æ·±åº¦ SSL ä¿®å¤)"""
    if not product_name:
        product_name = product_code
    url = "https://www.cmbcwm.com.cn/gw/po_web/BTADailyQry"
    headers = {'User-Agent': 'Mozilla/5.0'}
    start_date = (datetime.datetime.now() - timedelta(days=1460)).strftime("%Y%m%d")
    payload = {'chart_type': '0', 'real_prd_code': product_code, 'begin_date': start_date, 'end_date': ''}

    try:
        # === å…³é”®ä¿®å¤ï¼šæŒ‚è½½ Adapter ===
        session = requests.Session()
        session.mount('https://', LegacySSLAdapter())

        # verify=False ä»ç„¶ä¿ç•™ï¼Œä½†ç°åœ¨ adapter å†…éƒ¨å·²ç»å¤„ç†å¥½äº† check_hostname=False
        res = session.post(url, headers=headers, data=payload, verify=False, timeout=15)
        # ===========================

        nav_list = res.json().get('list', [])
        clean = []
        for i in nav_list:
            try:
                clean.append({'date': datetime.datetime.strptime(str(i['ISS_DATE']), "%Y%m%d").date(), 'nav': float(i['NAV'])})
            except: continue
        clean.sort(key=lambda x: x['date'])
        if not clean: return product_name, "No Data", "No Data", None, 0, 0

        last = clean[-1]
        prior = get_30_day_prior_record(clean, last['date'])
        return product_name, last['nav'], prior['nav'] if prior else 0, last['date'], get_nav_for_date(clean, purchase_date), get_nav_for_date(clean, redeem_date)
    except Exception as e:
        print(f"æ°‘ç”Ÿå¼‚å¸¸: {e}")
        return product_name, "Error", "Error", None, 0, 0

def query_efunds_yizeng(product_code, purchase_date=None, redeem_date=None):
    """æ˜“æ–¹è¾¾ (æ•´åˆäº†æ–°ç‰ˆé€»è¾‘ï¼šåˆå¹¶å†å²ä¸è¿‘æœŸæ•°æ®ï¼Œä½¿ç”¨æ­£åˆ™è§£æ)"""
    url_history = f'https://cdn.efunds.com.cn/market/2.0/his/{product_code}_all.js'
    url_recent = f'https://cdn.efunds.com.cn/market/2.0/{product_code}_1y.js'

    headers = {
        'Referer': 'https://www.efunds.com.cn/',
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36'
    }

    session = requests.Session()
    session.headers.update(headers)
    merged_data = {}

    try:
        for url in [url_history, url_recent]:
            try:
                res = session.get(url, timeout=10)
                # ä½¿ç”¨æ­£åˆ™æå–å¼•å·å†…çš„å†…å®¹
                match = re.search(r'=\s*"(.*?)";', res.text)
                if match:
                    content = match.group(1)
                    records = content.split(';')
                    for record in records:
                        if not record or '_' not in record or record.startswith('0_'):
                            continue
                        parts = record.split('_')
                        if len(parts) >= 3:
                            # parts[0]: YYYYMMDD, parts[2]: NAV
                            d_str = parts[0]
                            nav_val = float(parts[2])
                            dt = datetime.datetime.strptime(d_str, "%Y%m%d").date()
                            merged_data[dt] = nav_val
            except Exception as e:
                print(f"æ˜“æ–¹è¾¾ URL è¯·æ±‚é”™è¯¯ {url}: {e}")
                continue

        # è½¬æ¢ä¸ºåˆ—è¡¨å¹¶æ’åº
        clean = [{'date': k, 'nav': v} for k, v in merged_data.items()]
        clean.sort(key=lambda x: x['date'])

        if not clean:
            return product_code, "No Data", "No Data", None, 0, 0

        last = clean[-1]
        prior = get_30_day_prior_record(clean, last['date'])

        return product_code, last['nav'], prior['nav'] if prior else 0, last['date'], get_nav_for_date(clean, purchase_date), get_nav_for_date(clean, redeem_date)

    except Exception as e:
        print(f"æ˜“æ–¹è¾¾å¤„ç†å¼‚å¸¸: {e}")
        return product_code, "Error", "Error", None, 0, 0

def query_citic_wealth(product_code, purchase_date=None, redeem_date=None):
    """ä¸­ä¿¡é“¶è¡Œ (å®‰ç›ˆè±¡) - æ•´åˆè‡ª å®‰ç›ˆè±¡...py"""
    url = "https://wechat.citic-wealth.com/cms.product/api/custom/productInfo/getTAProductNav"
    params = {
        "prodCode": product_code,
        "queryUnit": "5" # æŸ¥è¯¢è¿‘5å¹´? æˆ–è€…å•ä½ï¼ŒåŸè„šæœ¬ä¸º5
    }
    
    headers = {
        "Accept": "application/json, text/plain, */*",
        "Origin": "https://www.citic-wealth.com",
        "Referer": "https://www.citic-wealth.com/",
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36",
        "channel": "h5_trade_service"
    }
    
    # æ³¨æ„ï¼šJSESSIONID å¯èƒ½ä¼šè¿‡æœŸï¼Œå¦‚æœå¤±æ•ˆéœ€è¦åœ¨ GitHub Secrets æˆ–é…ç½®æ–‡ä»¶ä¸­æ›´æ–°
    cookies = {
        "JSESSIONID": "rumlKZY0aDXcnkUn8dvr_lIyapa4KANRZMNXpQ3o"
    }

    try:
        # ä½¿ç”¨ LegacySSLAdapter ä»¥é˜²ä¸‡ä¸€ï¼Œè™½ç„¶ä¸­ä¿¡ä¸€èˆ¬ä¸éœ€è¦
        session = requests.Session()
        session.mount('https://', LegacySSLAdapter())
        
        response = session.get(url, headers=headers, params=params, cookies=cookies, verify=False, timeout=10)
        data = response.json()
        
        if data.get("code") != "0000":
            print(f"ä¸­ä¿¡ API é”™è¯¯ ({product_code}): {data.get('msg')}")
            return product_code, "API Error", 0, None, 0, 0

        nav_list = data.get("data", {}).get("productNavList", [])
        clean = []
        for item in nav_list:
            date_str = item.get("navDate")
            nav_value = item.get("nav")
            if date_str and nav_value is not None:
                try:
                    dt = datetime.datetime.strptime(date_str, "%Y%m%d").date()
                    clean.append({'date': dt, 'nav': float(nav_value)})
                except: continue
        
        clean.sort(key=lambda x: x['date'])
        
        if not clean:
            return product_code, "No Data", "No Data", None, 0, 0

        last = clean[-1]
        prior = get_30_day_prior_record(clean, last['date'])
        
        return product_code, last['nav'], prior['nav'] if prior else 0, last['date'], get_nav_for_date(clean, purchase_date), get_nav_for_date(clean, redeem_date)

    except Exception as e:
        print(f"ä¸­ä¿¡é“¶è¡Œå¼‚å¸¸ ({product_code}): {e}")
        return product_code, "Error", "Error", None, 0, 0

def query_hzbank(product_code, product_name=None, purchase_date=None, redeem_date=None):
    """æ­é“¶ç†è´¢"""
    if not product_name:
        product_name = product_code

    # å°†äº§å“ä»£ç è½¬æ¢ä¸ºå°å†™æ„å»ºURL
    product_code_lower = product_code.lower()
    url = f'http://www.hzbankwealth.cn/content/detail/{product_code_lower}_netval.json'

    headers = {
        'Accept': 'application/json, text/javascript, */*; q=0.01',
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        'Connection': 'keep-alive',
        'Referer': f'http://www.hzbankwealth.cn/content/detail/{product_code}.html',
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36',
        'X-Requested-With': 'XMLHttpRequest'
    }

    try:
        response = requests.get(url, headers=headers, verify=False, timeout=10)
        response.raise_for_status()
        data = response.json()

        if not data:
            return product_name, "No Data", "No Data", None, 0, 0

        # æ•°æ®æŒ‰æ—¥æœŸæ’åº
        date_fmt = "%Y-%m-%d"
        data.sort(key=lambda x: datetime.datetime.strptime(x['date'], date_fmt))

        # è·å–æœ€æ–°æ•°æ®
        latest_item = data[-1]
        latest_date_obj = datetime.datetime.strptime(latest_item['date'], date_fmt)
        latest_net_value = float(latest_item['net_value'])

        # è®¡ç®—30å¤©å‰çš„æ—¥æœŸ
        target_date_obj = latest_date_obj - datetime.timedelta(days=30)

        # æŸ¥æ‰¾30å¤©å‰çš„æ•°æ®ï¼ˆé¡ºå»¶é€»è¾‘ï¼‰
        comparison_item = None
        for item in data:
            current_item_date = datetime.datetime.strptime(item['date'], date_fmt)
            if current_item_date >= target_date_obj:
                comparison_item = item
                break

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œä½¿ç”¨ç¬¬ä¸€æ¡æ•°æ®
        if not comparison_item and data:
            comparison_item = data[0]

        prior_nav = float(comparison_item['net_value']) if comparison_item else 0

        # æ„é€ æ•°æ®æ ¼å¼
        clean_data = [{'date': datetime.datetime.strptime(item['date'], date_fmt).date(), 'nav': float(item['net_value'])} for item in data]

        return (product_name, latest_net_value, prior_nav, latest_date_obj.date(),
                get_nav_for_date(clean_data, purchase_date),
                get_nav_for_date(clean_data, redeem_date))

    except Exception as e:
        print(f"æ­é“¶ç†è´¢å¼‚å¸¸ ({product_code}): {e}")
        return product_name, "Error", "Error", None, 0, 0

def query_boc_niannianxin(purchase_date=None, redeem_date=None):
    """ä¸­è¡Œ"""
    code, name = "2501240100", "å¹´å¹´é‘«æœ€çŸ­æŒæœ‰æœŸ11å·A"
    url = "https://www.bankofchina.com/sourcedb/srfd6_2024/index_2.html"
    try:
        res = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=10)
        res.encoding = 'utf-8'
        soup = BeautifulSoup(res.text, 'html.parser')
        for row in soup.find_all('tr'):
            cols = row.find_all('td')
            if len(cols) >= 3 and cols[1].get_text(strip=True) == name:
                return code, float(cols[2].get_text(strip=True)), 0, datetime.date.today(), 0, 0
    except: pass
    return code, "Error", 0, None, 0, 0

# ==========================================
# ä¸»ç¨‹åº
# ==========================================
def main():
    print("ğŸš€ å¼€å§‹è¿è¡Œ...")
    info_map = load_purchase_dates("è´­å…¥æ—¥æœŸ.txt")
    feishu = FeishuClient(FEISHU_CONFIG["APP_ID"], FEISHU_CONFIG["APP_SECRET"])

    tasks = []

    # 1. äº¤è¡Œäº§å“
    print("ğŸ“‚ åŠ è½½äº¤è¡Œäº§å“ä»£ç ...")
    bocom_codes = load_product_codes("äº¤è¡Œäº§å“ä»£ç .txt")
    for c in bocom_codes:
        dates_list = info_map.get(c, [])
        if dates_list:
            # æœ‰è´­ä¹°æ—¥æœŸï¼Œæ¯ä¸ªæ—¥æœŸéƒ½ç”Ÿæˆä¸€ä¸ªä»»åŠ¡
            for d in dates_list:
                tasks.append((query_bocom(c, d.get('confirm_date'), d.get('redeem_date')), d.get('confirm_date')))
        else:
            # æ²¡æœ‰è´­ä¹°æ—¥æœŸï¼Œä»ç„¶è¦æŸ¥è¯¢ï¼ˆè·å–æœ€æ–°å‡€å€¼ï¼‰
            tasks.append((query_bocom(c, None, None), None))

    # 2. æ°‘ç”Ÿäº§å“
    print("ğŸ“‚ åŠ è½½æ°‘ç”Ÿäº§å“ä»£ç ...")
    cmbc_codes = load_product_codes("æ°‘ç”Ÿäº§å“ä»£ç .txt")
    for c in cmbc_codes:
        dates_list = info_map.get(c, [])
        if dates_list:
            for d in dates_list:
                tasks.append((query_cmbc_fuzhu(c, c, d.get('confirm_date'), d.get('redeem_date')), d.get('confirm_date')))
        else:
            tasks.append((query_cmbc_fuzhu(c, c, None, None), None))

    # 3. æ˜“æ–¹è¾¾äº§å“
    print("ğŸ“‚ åŠ è½½æ˜“æ–¹è¾¾äº§å“ä»£ç ...")
    efunds_codes = load_product_codes("æ˜“æ–¹è¾¾äº§å“ä»£ç .txt")
    for c in efunds_codes:
        dates_list = info_map.get(c, [])
        if dates_list:
            for d in dates_list:
                tasks.append((query_efunds_yizeng(c, d.get('confirm_date'), d.get('redeem_date')), d.get('confirm_date')))
        else:
            tasks.append((query_efunds_yizeng(c, None, None), None))

    # 4. ä¸­ä¿¡é“¶è¡Œäº§å“
    print("ğŸ“‚ åŠ è½½ä¸­ä¿¡é“¶è¡Œäº§å“ä»£ç ...")
    citic_codes = load_product_codes("ä¸­ä¿¡é“¶è¡Œäº§å“ä»£ç .txt")
    for c in citic_codes:
        dates_list = info_map.get(c, [])
        if dates_list:
            for d in dates_list:
                tasks.append((query_citic_wealth(c, d.get('confirm_date'), d.get('redeem_date')), d.get('confirm_date')))
        else:
            tasks.append((query_citic_wealth(c, None, None), None))

    # 5. æ­é“¶äº§å“
    print("ğŸ“‚ åŠ è½½æ­é“¶äº§å“ä»£ç ...")
    hzbank_codes = load_product_codes("æ­é“¶äº§å“ä»£ç .txt")
    for c in hzbank_codes:
        dates_list = info_map.get(c, [])
        if dates_list:
            for d in dates_list:
                tasks.append((query_hzbank(c, c, d.get('confirm_date'), d.get('redeem_date')), d.get('confirm_date')))
        else:
            tasks.append((query_hzbank(c, c, None, None), None))

    # 6. ä¸­è¡Œ (å›ºå®šäº§å“)
    print("ğŸ“‚ åŠ è½½ä¸­è¡Œäº§å“...")
    boc_dates_list = info_map.get("2501240100", [])
    if boc_dates_list:
        for d in boc_dates_list:
            tasks.append((query_boc_niannianxin(d.get('confirm_date'), d.get('redeem_date')), d.get('confirm_date')))
    else:
        tasks.append((query_boc_niannianxin(None, None), None))

    # ==========================================
    # åœ¨å†™å…¥æ•°æ®å‰ï¼Œå…ˆæ¸…ç©ºè¡¨æ ¼
    # ==========================================
    print("\nğŸ§¹ æ¸…ç©ºé£ä¹¦è¡¨æ ¼...")
    if not feishu.clear_table(FEISHU_CONFIG["APP_TOKEN"], FEISHU_CONFIG["TABLE_ID"]):
        print("âŒ æ¸…ç©ºè¡¨æ ¼å¤±è´¥ï¼Œç¨‹åºç»ˆæ­¢")
        return
    print()

    # æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡å¹¶å†™å…¥é£ä¹¦
    print("ğŸ“¤ å¼€å§‹å†™å…¥æ•°æ®åˆ°é£ä¹¦...")
    for (res, specific_c_date) in tasks:
        code, cur, prior, date_obj, pur, red = res
        if isinstance(cur, (int, float)) and date_obj:
            ts = int(datetime.datetime.combine(date_obj, datetime.time.min).timestamp() * 1000)
            c_ts = int(datetime.datetime.combine(specific_c_date, datetime.time.min).timestamp() * 1000) if specific_c_date else None

            fields = {
                "äº§å“ä»£ç ": code,
                "å½“æ—¥å‡€å€¼": cur,
                "30æ—¥å‰å‡€å€¼": prior,
                "è´­å…¥å½“æ—¥å‡€å€¼": pur,
                "èµå›å‡€å€¼": red if isinstance(red, (int, float)) else 0,
                "ç¡®è®¤æ—¥": c_ts,
                "æ•°æ®æ›´æ–°æ—¥æœŸ": ts
            }
            feishu.add_record(FEISHU_CONFIG["APP_TOKEN"], FEISHU_CONFIG["TABLE_ID"], fields)
        else:
            print(f"âš ï¸ è·³è¿‡: {code} (è·å–å¤±è´¥æˆ–æ ¼å¼é”™è¯¯)")

    print("\nâœ… æ‰€æœ‰æ•°æ®å†™å…¥å®Œæˆï¼")

if __name__ == "__main__":
    main()
